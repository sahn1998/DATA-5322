{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43011ce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2d7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667ec0a",
   "metadata": {},
   "source": [
    "# Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98bde69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All bird species available in the HDF5 spectrogram dataset\n",
    "ALL_BIRD_SPECIES = [\n",
    "    'amecro',  # American Crow\n",
    "    'amerob',  # American Robin\n",
    "    'bewwre',  # Bewick's Wren\n",
    "    'bkcchi',  # Black-capped Chickadee\n",
    "    'daejun',  # Dark-eyed Junco\n",
    "    'houfin',  # House Finch\n",
    "    'houspa',  # House Sparrow\n",
    "    'norfli',  # Northern Flicker\n",
    "    'rewbla',  # Red-winged Blackbird\n",
    "    'sonspa',  # Song Sparrow\n",
    "    'spotow',  # Spotted Towhee\n",
    "    'whcspa',  # White-crowned Sparrow\n",
    "]\n",
    "\n",
    "# Selected bird species for binary classification\n",
    "# Class 0: 'houspa' â†’ House Sparrow\n",
    "# Class 1: 'sonspa' â†’ Song Sparrow\n",
    "SELECTED_BIRD_CLASSES = [\n",
    "    'houspa',  # Class 0\n",
    "    'sonspa',  # Class 1\n",
    "]\n",
    "\n",
    "# Path to the HDF5 file containing bird spectrograms\n",
    "SPEC_FILE_PATH = Path('../data/bird_spectrograms.hdf5')\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 50                 # Total number of training epochs\n",
    "NUM_CV_FOLDS = 3            # K-Fold cross-validation (choose 3, 4, or 5)\n",
    "BATCH_SIZE = 16             # Batch size for training\n",
    "LEARNING_RATE = 0.001      # Learning rate for the optimizer\n",
    "\n",
    "# List of evaluation metrics returned by `evaluate()` function\n",
    "EVALUATION_METRICS = [\n",
    "    \"Weighted Accuracy\",          # Balanced accuracy between houspa recall and sonspa specificity\n",
    "    \"Sensitivity/Recall\",         # Recall for houspa (class 0)\n",
    "    \"Specificity\",                # Specificity for sonspa (class 1)\n",
    "    \"Precision_houspa\",           # Precision for class 0\n",
    "    \"Precision_sonspa\",           # Precision for class 1\n",
    "    \"Precision_avg\",              # Average of both precisions\n",
    "    \"F1_houspa\",                  # F1 score for class 0\n",
    "    \"F1_sonspa\",                  # F1 score for class 1\n",
    "    \"F1_avg\",                     # Mean of both F1 scores\n",
    "    \"AUC_ROC_Score\",              # Approximate ROC AUC\n",
    "    \"False_Discovery_Rate\",       # FP / (FP + TP) for sonspa\n",
    "    \"False_Negative_Rate\",        # FN / (FN + TP) for houspa\n",
    "    \"False_Omission_Rate\",        # FN / (FN + TN)\n",
    "    \"False_Positive_Rate\",        # FP / (FP + TN) for sonspa\n",
    "    \"Jaccard_Index\"               # Intersection-over-union for houspa\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501b2e0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "We need to transpose the data from (128, 517, sample_size) into (sample_size, 128, 517) because CNNs expect an input of shape (N, C, H, W). \n",
    "\n",
    "Here:\n",
    "- N = number of samples\n",
    "- C = number of channels (1 for grayscale spectrograms)\n",
    "- H = height (128)\n",
    "- W = width (517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72b123e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "with h5py.File(SPEC_FILE_PATH, 'r') as f:\n",
    "    for label, key in enumerate(SELECTED_BIRD_CLASSES):\n",
    "        data = f[key][:]  # shape = (128, 517, N)\n",
    "        data = np.transpose(data, (2, 0, 1))  # shape = (N, 128, 517)\n",
    "        X.append(data)\n",
    "        y.append(np.full(data.shape[0], label))  # 0 for houspa, 1 for sonspa\n",
    "\n",
    "X = np.concatenate(X, axis=0)  # (N_total, 128, 517)\n",
    "y = np.concatenate(y)\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959d1a2",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6fd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_hat_class, Y):\n",
    "    \"\"\"\n",
    "    Evaluate binary classification metrics for Houspa (class 0) vs. Sonspa (class 1).\n",
    "\n",
    "    Parameters:\n",
    "    y_hat_class (np.ndarray): Predicted class labels (0=houspa, 1=sonspa).\n",
    "    Y (np.ndarray): True class labels.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (metrics array, confusion matrix)\n",
    "    \"\"\"\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "    y_hat_class = y_hat_class.reshape(-1)\n",
    "\n",
    "    # Build confusion matrix\n",
    "    for y_hat, y in zip(y_hat_class, Y):\n",
    "        if y == 0:  # Actual houspa\n",
    "            if y_hat == 0:\n",
    "                cm[0, 0] += 1  # True Positive (TP) for houspa\n",
    "            else:\n",
    "                cm[0, 1] += 1  # False Negative (FN): missed houspa\n",
    "        elif y == 1:  # Actual sonspa\n",
    "            if y_hat == 1:\n",
    "                cm[1, 1] += 1  # True Positive (TP) for sonspa\n",
    "            else:\n",
    "                cm[1, 0] += 1  # False Positive (FP): incorrectly predicted houspa\n",
    "\n",
    "    # Confusion matrix entries\n",
    "    TP_houspa = cm[0, 0]\n",
    "    FN_houspa = cm[0, 1]\n",
    "    FP_sonspa = cm[1, 0]\n",
    "    TP_sonspa = cm[1, 1]\n",
    "\n",
    "    # Sensitivity/Recall for houspa\n",
    "    recall_houspa = TP_houspa / (TP_houspa + FN_houspa) if (TP_houspa + FN_houspa) != 0 else 0.0\n",
    "    # Specificity for sonspa\n",
    "    specificity_sonspa = TP_sonspa / (TP_sonspa + FP_sonspa) if (TP_sonspa + FP_sonspa) != 0 else 0.0\n",
    "\n",
    "    # Precision for each class\n",
    "    precision_houspa = TP_houspa / (TP_houspa + FP_sonspa) if (TP_houspa + FP_sonspa) != 0 else 0.0\n",
    "    precision_sonspa = TP_sonspa / (TP_sonspa + FN_houspa) if (TP_sonspa + FN_houspa) != 0 else 0.0\n",
    "\n",
    "    # F1 Scores\n",
    "    f1_houspa = 2 * precision_houspa * recall_houspa / (precision_houspa + recall_houspa) if (precision_houspa + recall_houspa) != 0 else 0.0\n",
    "    f1_sonspa = 2 * precision_sonspa * specificity_sonspa / (precision_sonspa + specificity_sonspa) if (precision_sonspa + specificity_sonspa) != 0 else 0.0\n",
    "\n",
    "    # Averages\n",
    "    precision_avg = (precision_houspa + precision_sonspa) / 2\n",
    "    f1_avg = (f1_houspa + f1_sonspa) / 2\n",
    "    weighted_accuracy = 0.5 * (recall_houspa + specificity_sonspa)\n",
    "\n",
    "    # ROC AUC (approx.)\n",
    "    auc_roc = 0.5 * (1 + recall_houspa - (FP_sonspa / (FP_sonspa + TP_sonspa) if (FP_sonspa + TP_sonspa) != 0 else 0.0))\n",
    "\n",
    "    # Jaccard index for class 0 (houspa)\n",
    "    jaccard = TP_houspa / (TP_houspa + FN_houspa + FP_sonspa) if (TP_houspa + FN_houspa + FP_sonspa) != 0 else 0.0\n",
    "\n",
    "    # Error Rates\n",
    "    fdr = FP_sonspa / (FP_sonspa + TP_houspa) if (FP_sonspa + TP_houspa) != 0 else 0.0  # False Discovery Rate\n",
    "    fnr = FN_houspa / (FN_houspa + TP_houspa) if (FN_houspa + TP_houspa) != 0 else 0.0  # False Negative Rate\n",
    "    forate = FN_houspa / (FN_houspa + TP_sonspa) if (FN_houspa + TP_sonspa) != 0 else 0.0  # False Omission Rate\n",
    "    fpr = FP_sonspa / (FP_sonspa + TP_sonspa) if (FP_sonspa + TP_sonspa) != 0 else 0.0  # False Positive Rate\n",
    "\n",
    "    return (\n",
    "        np.array([\n",
    "            weighted_accuracy,   # 0\n",
    "            recall_houspa,       # 1\n",
    "            specificity_sonspa,  # 2\n",
    "            precision_houspa,    # 3\n",
    "            precision_sonspa,    # 4\n",
    "            precision_avg,       # 5\n",
    "            f1_houspa,           # 6\n",
    "            f1_sonspa,           # 7\n",
    "            f1_avg,              # 8\n",
    "            auc_roc,             # 9\n",
    "            fdr,                 # 10\n",
    "            fnr,                 # 11\n",
    "            forate,              # 12\n",
    "            fpr,                 # 13\n",
    "            jaccard              # 14\n",
    "        ]),\n",
    "        cm\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b7055",
   "metadata": {},
   "source": [
    "# CNN Model Model Architecture\n",
    "\n",
    "This convolutional neural network (CNN) performs **binary classification** on spectrogram inputs of shape `(1, 128, 517)`, where:\n",
    "- `1` is the channel dimension (grayscale),\n",
    "- `128` is the number of frequency bins (height),\n",
    "- `517` is the number of time steps (width).\n",
    "\n",
    "---\n",
    "\n",
    "### Model Layers:\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "```\n",
    "\n",
    "- Applies 16 convolutional filters of size 3Ã—3.\n",
    "- Padding keeps the spatial size unchanged (128Ã—517).\n",
    "\n",
    "```python\n",
    "self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "```\n",
    "\n",
    "- Applies 32 filters to the output of the first conv layer.\n",
    "- Extracts more abstract audio patterns.\n",
    "\n",
    "```python\n",
    "self.pool = nn.MaxPool2d(2)\n",
    "```\n",
    "\n",
    "- Reduces both height and width by a factor of 2.\n",
    "-  After two pooling layers: (128, 517) â†’ (64, 258) â†’ (32, 129)\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(32 * 32 * 129, 64)\n",
    "```\n",
    "\n",
    "- Fully connected layer with 131,072 input features.\n",
    "- Reduces to a 64-dimensional feature vector.\n",
    "\n",
    "```python\n",
    "self.fc2 = nn.Linear(64, 2)\n",
    "```\n",
    "\n",
    "- Outputs 2 logits (one per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdBinaryCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Global Average Pooling reduces spatial dimension to 1x1\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # Output shape: (batch, 32, 1, 1)\n",
    "\n",
    "        # Final classifier\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # (16, 64, 258)\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # (32, 32, 129)\n",
    "        x = self.gap(x)                                 # (32, 1, 1)\n",
    "        x = x.view(x.size(0), -1)                       # Flatten to (32,)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06260f4d",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Loop\n",
    "\n",
    "For CNNs, these four essential steps are important. <br>\n",
    "I have explained more about backpropagation in this [Medium article](https://medium.com/data-science-collective/why-backpropagation-is-so-important-for-models-in-machine-learning-4736591b24b3)\n",
    "\n",
    "- Forward pass: Feed input through the network to make a prediction.\n",
    "- Loss calculation: It compares those predictions to the actual labels and computes the loss.\n",
    "- Backpropagation: Use the chain rule to find how much each weight influenced the loss.\n",
    "- Weight update: Apply the gradients using an optimizer like Adam.\n",
    "---\n",
    "\n",
    "## Reasoning behind of KFold Cross Validation\n",
    "Since the dataset is relatively small, it may be best to go with just a KFold Cross Validation rather than doing a train-test split + cross-validation. <br>\n",
    "That is the reason why I went with K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4639dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1 | Train size: 595, Test size: 298\n",
      "Epoch 1/50 | Train Loss: 24.2120 | Val Loss: 11.2807\n",
      "Epoch 2/50 | Train Loss: 23.7231 | Val Loss: 11.2279\n",
      "Epoch 3/50 | Train Loss: 23.5185 | Val Loss: 11.1479\n",
      "Epoch 4/50 | Train Loss: 23.3036 | Val Loss: 11.1335\n",
      "Epoch 5/50 | Train Loss: 23.8738 | Val Loss: 11.2174\n",
      "Epoch 6/50 | Train Loss: 23.5011 | Val Loss: 11.0833\n",
      "Epoch 7/50 | Train Loss: 23.0533 | Val Loss: 11.1134\n",
      "Epoch 8/50 | Train Loss: 23.8013 | Val Loss: 11.3719\n",
      "Epoch 9/50 | Train Loss: 23.6421 | Val Loss: 11.1173\n",
      "Epoch 10/50 | Train Loss: 23.0470 | Val Loss: 11.0010\n",
      "Epoch 11/50 | Train Loss: 23.6704 | Val Loss: 11.0561\n",
      "Epoch 12/50 | Train Loss: 23.2830 | Val Loss: 10.9745\n",
      "Epoch 13/50 | Train Loss: 23.7138 | Val Loss: 11.1007\n",
      "Epoch 14/50 | Train Loss: 23.0620 | Val Loss: 11.1119\n",
      "Epoch 15/50 | Train Loss: 23.4806 | Val Loss: 11.1363\n",
      "Epoch 16/50 | Train Loss: 23.5131 | Val Loss: 10.9405\n",
      "Epoch 17/50 | Train Loss: 23.6349 | Val Loss: 10.9990\n",
      "Epoch 18/50 | Train Loss: 22.8798 | Val Loss: 10.8291\n",
      "Epoch 19/50 | Train Loss: 23.4090 | Val Loss: 10.9913\n",
      "Epoch 20/50 | Train Loss: 22.5045 | Val Loss: 11.6749\n",
      "Epoch 21/50 | Train Loss: 23.3046 | Val Loss: 10.8232\n",
      "Epoch 22/50 | Train Loss: 23.0054 | Val Loss: 10.8620\n",
      "Epoch 23/50 | Train Loss: 22.9653 | Val Loss: 10.6981\n",
      "Epoch 24/50 | Train Loss: 22.9258 | Val Loss: 11.2146\n",
      "Epoch 25/50 | Train Loss: 22.9913 | Val Loss: 11.2921\n",
      "Epoch 26/50 | Train Loss: 23.1195 | Val Loss: 11.6027\n",
      "Epoch 27/50 | Train Loss: 23.3288 | Val Loss: 10.6315\n",
      "Epoch 28/50 | Train Loss: 22.8298 | Val Loss: 10.9032\n",
      "Epoch 29/50 | Train Loss: 22.5156 | Val Loss: 11.0983\n",
      "Epoch 30/50 | Train Loss: 22.8491 | Val Loss: 12.5774\n",
      "Epoch 31/50 | Train Loss: 22.5533 | Val Loss: 12.4946\n",
      "Epoch 32/50 | Train Loss: 22.3680 | Val Loss: 10.8162\n",
      "Epoch 33/50 | Train Loss: 22.7878 | Val Loss: 11.8029\n",
      "Epoch 34/50 | Train Loss: 22.7986 | Val Loss: 10.4292\n",
      "Epoch 35/50 | Train Loss: 23.4891 | Val Loss: 10.4853\n",
      "Epoch 36/50 | Train Loss: 22.4109 | Val Loss: 10.5384\n",
      "Epoch 37/50 | Train Loss: 23.3187 | Val Loss: 10.7060\n",
      "Epoch 38/50 | Train Loss: 22.2994 | Val Loss: 10.9332\n",
      "Epoch 39/50 | Train Loss: 22.2480 | Val Loss: 10.6701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m loss = criterion(pred, yb)\n\u001b[32m     35\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m optimizer.step()\n\u001b[32m     38\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sahn1\\Documentos\\2025 Spring Quarter\\DATA-5322\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sahn1\\Documentos\\2025 Spring Quarter\\DATA-5322\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sahn1\\Documentos\\2025 Spring Quarter\\DATA-5322\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#  Convert NumPy to Torch Tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # (N_total, 1, 128, 517)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long) # Converts labels (e.g., [0, 0, 0, 1, 1, 1, ...]) to a PyTorch tensor\n",
    "\n",
    "# Train Test Split\n",
    "kfold = KFold(n_splits=NUM_CV_FOLDS, shuffle=True, random_state=42)\n",
    "metrics_all_folds = pd.DataFrame(columns=EVALUATION_METRICS)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X_tensor)):\n",
    "    print(f\"\\n Fold {fold + 1} | Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "    # Step 1: Split the data according to folds\n",
    "    X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]\n",
    "    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]\n",
    "\n",
    "    # Step 2: Wrap in TensorDataset and DataLoader\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # Check Data Settings for BS\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE) # Check Data Settings for BS\n",
    "\n",
    "    # Step 3: Initialize model and optimizer\n",
    "    model = BirdBinaryCNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # Check Data Settings for LR\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation loss (only loss, no metrics)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS} | Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            pred = model(xb)\n",
    "            preds = pred.argmax(dim=1)\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    metrics, cm = evaluate(np.array(y_pred), np.array(y_true))\n",
    "    VERSION_TAG = f\"../output/binary_model_{EPOCHS}_fold_{fold}\"\n",
    "\n",
    "\n",
    "    # Save results per fold\n",
    "    metrics_all_folds.loc[VERSION_TAG] = list(metrics)\n",
    "\n",
    "    single_result_df = pd.DataFrame(\n",
    "        metrics_all_folds.loc[[VERSION_TAG]]\n",
    "    )\n",
    "\n",
    "    output_path = os.path.join(f\"{VERSION_TAG}_student_evaluation_results.csv\")\n",
    "\n",
    "    # Save in append mode, include header only if file does not exist\n",
    "    if os.path.isfile(output_path):\n",
    "        single_result_df.to_csv(output_path, mode=\"a\", header=False)\n",
    "    else:\n",
    "        single_result_df.to_csv(output_path, mode=\"a\", header=True)\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# === Final Summary ===\n",
    "print(\"\\nðŸ“Š Cross-Validation Summary:\")\n",
    "print(f\"Avg Sensitivity (Recall):  {metrics_all_folds['Sensitivity/Recall'].astype(float).mean():.4f}\")\n",
    "print(f\"Avg Specificity:           {metrics_all_folds['Specificity'].astype(float).mean():.4f}\")\n",
    "print(f\"Avg Weighted Accuracy:     {metrics_all_folds['Weighted Accuracy'].astype(float).mean():.4f}\")\n",
    "print(f\"Avg F1 Score (Avg):         {metrics_all_folds['F1_avg'].astype(float).mean():.4f}\")\n",
    "print(f\"Avg AUC-ROC:               {metrics_all_folds['auc_roc_score'].astype(float).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bde66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
