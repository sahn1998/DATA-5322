{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43011ce5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:32:27.034125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746646347.051874    9824 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746646347.057222    9824 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746646347.073855    9824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746646347.073870    9824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746646347.073873    9824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746646347.073874    9824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 19:32:27.079378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from model_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd714adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 out of 16 available CPU cores (75%)\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Configuration\n",
    "import multiprocessing\n",
    "\n",
    "# Get the total number of CPU cores available\n",
    "total_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Calculate 75% of available cores (rounded down)\n",
    "cores_to_use = int(total_cores * 0.75)\n",
    "\n",
    "# Ensure at least 1 core is used\n",
    "cores_to_use = max(1, cores_to_use)\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(cores_to_use)\n",
    "\n",
    "print(f\"Using {cores_to_use} out of {total_cores} available CPU cores (75%)\")\n",
    "\n",
    "# Add after TensorFlow import\n",
    "tf.config.threading.set_inter_op_parallelism_threads(cores_to_use // 2)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(cores_to_use // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667ec0a",
   "metadata": {},
   "source": [
    "# Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bde69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All bird species available in the HDF5 spectrogram dataset\n",
    "ALL_BIRD_SPECIES = [\n",
    "    'amecro',  # American Crow\n",
    "    'amerob',  # American Robin\n",
    "    'bewwre',  # Bewick's Wren\n",
    "    'bkcchi',  # Black-capped Chickadee\n",
    "    'daejun',  # Dark-eyed Junco\n",
    "    'houfin',  # House Finch\n",
    "    'houspa',  # House Sparrow\n",
    "    'norfli',  # Northern Flicker\n",
    "    'rewbla',  # Red-winged Blackbird\n",
    "    'sonspa',  # Song Sparrow\n",
    "    'spotow',  # Spotted Towhee\n",
    "    'whcspa',  # White-crowned Sparrow\n",
    "]\n",
    "\n",
    "# Path to the HDF5 file containing bird spectrograms\n",
    "SPEC_FILE_PATH = Path('../data/bird_spectrograms.hdf5')\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 50                 # Total number of training epochs\n",
    "NUM_CV_FOLDS = 3            # K-Fold cross-validation (choose 3, 4, or 5)\n",
    "BATCH_SIZE = [64, 128, 32]           # Batch size for training\n",
    "LEARNING_RATE = [0.0001, 0.0005, 0.001]      # Learning rate for the optimizer\n",
    "\n",
    "# List of evaluation metrics\n",
    "EVALUATION_METRICS = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501b2e0",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "We need to transpose the data from (128, 517, sample_size) into (sample_size, 128, 517) because CNNs expect an input of shape (N, C, H, W). \n",
    "\n",
    "Here:\n",
    "- N = number of samples\n",
    "- C = number of channels (1 for grayscale spectrograms)\n",
    "- H = height (128)\n",
    "- W = width (517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(893, 128, 517, 1)\n",
      "(893, 1)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "with h5py.File(SPEC_FILE_PATH, 'r') as f:\n",
    "    for label, key in enumerate(f.keys()):\n",
    "        data = f[key][:]  # shape = (128, 517, N)\n",
    "        data = np.transpose(data, (2, 0, 1))  # shape = (N, 128, 517)\n",
    "        X.append(data)\n",
    "        y.append(np.full((data.shape[0],), label, dtype=np.int32))\n",
    "\n",
    "X = np.concatenate(X, axis=0)  # (N_total, 128, 517)\n",
    "y = np.concatenate(y)          # (N_total,)\n",
    "\n",
    "X = np.expand_dims(X, axis=-1)  # (N_total, 128, 517, 1)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b7055",
   "metadata": {},
   "source": [
    "# CNN Model Model Architecture\n",
    "\n",
    "This convolutional neural network (CNN) performs **binary classification** on spectrogram inputs of shape `(1, 128, 517)`, where:\n",
    "- `1` is the channel dimension (grayscale),\n",
    "- `128` is the number of frequency bins (height),\n",
    "- `517` is the number of time steps (width).\n",
    "\n",
    "---\n",
    "\n",
    "### General Steps for CNN\n",
    "According to a very long discussion on the order of layers, BN, DropOut, and Pooling on [stackoverflow](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout) <br>\n",
    "Generally, this laying is the consensus. <br>\n",
    "\n",
    "Conv → BatchNorm → ReLU → Dropout → MaxPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35111290",
   "metadata": {},
   "source": [
    "# CNN Model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bird_binary_cnn_v1(num_classes, input_shape=(128, 517, 1), dropout_rate=0.2, learning_rate=0.0001):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_bird_binary_cnn_v2(num_classes, input_shape=(128, 517, 1), dropout_rate=0.2, learning_rate=0.0001):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf6c2d",
   "metadata": {},
   "source": [
    "# Graph Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa25a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plot_training_history(history, path, tag):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracy and loss from two training histories.\n",
    "\n",
    "    Args:\n",
    "        history: First training history object.\n",
    "        history2: Second training history object.\n",
    "    \"\"\"\n",
    "    # Extract values from history object\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{path}/{tag}_acc.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{path}/{tag}_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06260f4d",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Loop\n",
    "\n",
    "For CNNs, these four essential steps are important. <br>\n",
    "I have explained more about backpropagation in this [Medium article](https://medium.com/data-science-collective/why-backpropagation-is-so-important-for-models-in-machine-learning-4736591b24b3)\n",
    "\n",
    "- Forward pass: Feed input through the network to make a prediction.\n",
    "- Loss calculation: It compares those predictions to the actual labels and computes the loss.\n",
    "- Backpropagation: Use the chain rule to find how much each weight influenced the loss.\n",
    "- Weight update: Apply the gradients using an optimizer like Adam.\n",
    "---\n",
    "\n",
    "## Reasoning behind of KFold Cross Validation\n",
    "Since the dataset is relatively small, it may be best to go with just a KFold Cross Validation rather than doing a train-test split + cross-validation. <br>\n",
    "\n",
    "That is the reason why I went with K-fold cross validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174a624",
   "metadata": {},
   "source": [
    "# 2 Layer Model Run\n",
    "- 3 FOLD (stratified)\n",
    "- Batch Size = 32\n",
    "- Dropout = 0.2\n",
    "- Learning Rate = [0.001, 0.005, 0.0001]\n",
    "- 50 EPOCHS with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold CV\n",
    "kfold = StratifiedKFold(n_splits=NUM_CV_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for batch_size in BATCH_SIZE:\n",
    "    for learning_rate in LEARNING_RATE:\n",
    "        version_key_prefix = f\"multi_2layer_{batch_size}_{learning_rate}_{EPOCHS}\"\n",
    "        result_path = f\"../output/multi-class_results\"\n",
    "        fold_results = []\n",
    "        metrics = pd.DataFrame(columns=EVALUATION_METRICS)\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "            print(f\"\\nFold {fold + 1} | Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            print(X_train.shape[1:])\n",
    "            # Initialize model\n",
    "            model = build_bird_binary_cnn_v1(input_shape=X_train.shape[1:], dropout_rate=0.2, learning_rate=learning_rate)\n",
    "\n",
    "            # Callbacks\n",
    "            early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "            # Train\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Optional: Training history visualization\n",
    "            build_plot_training_history(history)\n",
    "\n",
    "            # Evaluate\n",
    "            y_probs = model.predict(X_test)\n",
    "            y_pred = np.argmax(y_probs, axis=1)\n",
    "            y_true = y_test.flatten()\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "            # Classification Report\n",
    "            report = classification_report(y_true, y_pred, digits=4)\n",
    "            print(\"Classification Report:\\n\", report)\n",
    "\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred, average='macro')\n",
    "            rec = recall_score(y_true, y_pred, average='macro')\n",
    "            f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "            fold_tag = f\"{version_key_prefix}_fold_{fold + 1}\"\n",
    "            metrics.loc[fold_tag] = [acc, prec, rec, f1]\n",
    "\n",
    "            # Cleanup\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "        \n",
    "        # Save all 3 folds' metrics into a single CSV file\n",
    "        result_subset = metrics.loc[[f\"{version_key_prefix}_fold_{i+1}\" for i in range(NUM_CV_FOLDS)]]\n",
    "        output_path = f\"{result_path}/4layer_{version_key_prefix}_results.csv\"\n",
    "        result_subset.to_csv(output_path, index_label=\"Fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1824699",
   "metadata": {},
   "source": [
    "# 4 Layer Model Run\n",
    "- 3 FOLD (stratified)\n",
    "- Batch Size = 32\n",
    "- Dropout = 0.2\n",
    "- Learning Rate = [0.001, 0.005, 0.0001]\n",
    "- 50 EPOCHS with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 | Train size: 595, Test size: 298\n",
      "(128, 517, 1)\n",
      "Epoch 1/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.5769 - loss: 0.7374 - val_accuracy: 0.2953 - val_loss: 1.1499 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m 4/19\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.6947 - loss: 0.6040"
     ]
    }
   ],
   "source": [
    "# K-Fold CV\n",
    "kfold = StratifiedKFold(n_splits=NUM_CV_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for batch_size in BATCH_SIZE:\n",
    "    for learning_rate in LEARNING_RATE:\n",
    "        version_key_prefix = f\"multi_4layer_{batch_size}_{learning_rate}_{EPOCHS}\"\n",
    "        result_path = f\"../output/multi-class_results\"\n",
    "        fold_results = []\n",
    "        metrics = pd.DataFrame(columns=EVALUATION_METRICS)\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "            print(f\"\\nFold {fold + 1} | Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            print(X_train.shape[1:])\n",
    "            # Initialize model\n",
    "            model = build_bird_binary_cnn_v2(input_shape=X_train.shape[1:], dropout_rate=0.2, learning_rate=learning_rate)\n",
    "\n",
    "            # Callbacks\n",
    "            early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "            # Train\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Optional: Training history visualization\n",
    "            build_plot_training_history(history, result_path, fold_tag)\n",
    "\n",
    "             # Evaluate\n",
    "            y_probs = model.predict(X_test)\n",
    "            y_pred = np.argmax(y_probs, axis=1)\n",
    "            y_true = y_test.flatten()\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "            # Classification Report\n",
    "            report = classification_report(y_true, y_pred, digits=4)\n",
    "            print(\"Classification Report:\\n\", report)\n",
    "\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred, average='macro')\n",
    "            rec = recall_score(y_true, y_pred, average='macro')\n",
    "            f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "            fold_tag = f\"{version_key_prefix}_fold_{fold + 1}\"\n",
    "            metrics.loc[fold_tag] = [acc, prec, rec, f1]\n",
    "\n",
    "            # Cleanup\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "        \n",
    "        # Save all 3 folds' metrics into a single CSV file\n",
    "        result_subset = metrics.loc[[f\"{version_key_prefix}_fold_{i+1}\" for i in range(NUM_CV_FOLDS)]]\n",
    "        output_path = f\"{result_path}/4layer_{version_key_prefix}_results.csv\"\n",
    "        result_subset.to_csv(output_path, index_label=\"Fold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
